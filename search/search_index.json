{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#what-is-kube-burner","title":"What is Kube-burner","text":"<p>Kube-burner is a Kubernetes performance and scale test orchestration toolset. It provides multi-faceted functionality, the most important of which are summarized below.</p> <ul> <li>Create, delete, and patch Kubernetes resources at scale.</li> <li>Prometheus metric collection and indexing.</li> <li>Measurements.</li> <li>Alerting.</li> </ul> <p>Kube-burner is a binary application written in Golang that makes extensive usage of the official k8s client library, client-go.</p> <p></p>"},{"location":"#quick-starting-with-kube-burner","title":"Quick starting with kube-burner","text":"<p>To start tinkering with kube-burner now:</p> <ul> <li>Find binaries for different CPU architectures and operating systems in the releases section of the repository.</li> <li>Use the container image repository available at quay.</li> <li>Reference valid examples of configuration files, metrics profiles, and Grafana dashboards in the examples directory of the repository.</li> </ul>"},{"location":"cli/","title":"CLI","text":"<p>kube-burner is a tool written in Golang that can be used to stress Kubernetes clusters by creating, deleting, and patching resources at a given rate. The actions taken by this tool are highly customizable and their available subcommands are detailed below:</p> <pre><code>$ kube-burner help\nKube-burner \ud83d\udd25\n\nTool aimed at stressing a kubernetes cluster by creating or deleting lots of objects.\n\nUsage:\n  kube-burner [command]\n\nAvailable Commands:\n  check-alerts Evaluate alerts for the given time range\n  completion   Generates completion scripts for bash shell\n  destroy      Destroy old namespaces labeled with the given UUID.\n  help         Help about any command\n  import       Import metrics tarball\n  index        Index kube-burner metrics\n  init         Launch benchmark\n  measure      Take measurements for a given set of resources without running workload\n  ocp          OpenShift wrapper\n  version      Print the version number of kube-burner\n\nFlags:\n  -h, --help               help for kube-burner\n      --log-level string   Allowed values: debug, info, warn, error, fatal (default \"info\")\n\nUse \"kube-burner [command] --help\" for more information about a command.\n</code></pre>"},{"location":"cli/#init","title":"Init","text":"<p>This is the main subcommand; it triggers a new kube-burner benchmark and it supports the these flags:</p> <ul> <li><code>uuid</code>: Benchmark ID. This is essentially an arbitrary string that is used for different purposes along the benchmark. For example, label the objects created by kube-burner as mentioned in the reference chapter. By default, it is auto-generated.</li> <li><code>config</code>: Path or URL to a valid configuration file. See details about the configuration schema in the reference chapter.</li> <li><code>configmap</code>: In case of not providing the <code>--config</code> flag, kube-burner is able to fetch its configuration from a given <code>configMap</code>. This variable configures its name. kube-burner expects the configMap to hold all the required configuration: config.yml, metrics.yml, and alerts.yml. Where metrics.yml and alerts.yml are optional.</li> <li><code>namespace</code>: Name of the namespace where the configmap is.</li> <li><code>log-level</code>: Logging level, one of: <code>debug</code>, <code>error</code>, <code>info</code> or <code>fatal</code>. Default <code>info</code>.</li> <li><code>prometheus-url</code>: Prometheus endpoint, required for metrics collection. For example: <code>https://prometheus-k8s-openshift-monitoring.apps.rsevilla.stress.mycluster.example.com</code></li> <li><code>metrics-profile</code>: Path to a valid metrics profile file. The default is <code>metrics.yml</code>.</li> <li><code>metrics-endpoint</code>: Path to a valid metrics endpoint file.</li> <li><code>token</code>: Prometheus Bearer token.</li> <li><code>username</code>: Prometheus username for basic authentication.</li> <li><code>password</code>: Prometheus password for basic authentication.</li> <li><code>skip-tls-verify</code>: Skip TLS verification for Prometheus. The default is <code>true</code>.</li> <li><code>step</code>: Prometheus step size. The default is <code>30s</code>.</li> <li><code>timeout</code>: Kube-burner benchmark global timeout. When timing out, return code is 2. The default is <code>4h</code>.</li> <li><code>user-metadata</code>: YAML file path containing custom user-metadata to be indexed.</li> </ul> <p>Prometheus authentication</p> <p>Both basic and token authentication methods need permissions able to query the given Prometheus endpoint.</p> <p>With the above, running a kube-burner benchmark would be as simple as:</p> <pre><code>kube-burner init -c cfg.yml -u https://prometheus-k8s-openshift-monitoring.apps.rsevilla.stress.mycluster.example.com -t ${token} --uuid 67f9ec6d-6a9e-46b6-a3bb-065cde988790`\n</code></pre> <p>Kube-burner also supports remote configuration files served by a web server. To use it, rather than a path, pass a URL. For example:</p> <pre><code>kube-burner init -c http://web.domain.com:8080/cfg.yml -t ${token} --uuid 67f9ec6d-6a9e-46b6-a3bb-065cde988790`\n</code></pre> <p>To scrape metrics from multiple endpoints, the  <code>init</code> command can be triggered. For example:</p> <pre><code>kube-burner init -c cluster-density.yml -e metrics-endpoints.yaml\n</code></pre> <p>A metrics-endpoints.yaml file with valid keys for the <code>init</code> command would look like the following:</p> <pre><code>- endpoint: http://localhost:9090\n  token: &lt;token&gt;\n  profile: metrics.yaml\n  alertProfile: alert-profile.yaml\n- endpoint: http://remotehost:9090\n  token: &lt;token&gt;\n</code></pre> <p>Note</p> <p>Options <code>profile</code> and <code>alertProfile</code> are optional. If not provided, the options will be taken from the CLI flags first. Otherwise, they are populated with the default values. Invalid keys are ignored.</p>"},{"location":"cli/#index","title":"Index","text":"<p>This subcommand can be used to collect and index the metrics from a given time range. The time range is given by:</p> <ul> <li><code>start</code>: Epoch start time. Defaults to one hour before the current time.</li> <li><code>end</code>: Epoch end time. Defaults to the current time.</li> </ul>"},{"location":"cli/#measure","title":"Measure","text":"<p>This subcommand can be used to collect measurements for a given set of resources which were part of a workload ran in past and are still present on the cluster (i.e only supports podLatency as of today). We can specify a list of namespaces and selector labels as input.</p> <ul> <li><code>namespaces</code>: comma-separated list of namespaces provided as a string input. This is optional, by default all namespaces are considered.</li> <li><code>selector</code>: comma-separated list of selector labels in the format key1=value1,key2=value2. This is optional, by default no labels will be used for filtering.</li> </ul> <p>Note</p> <p>This subcommand should only be used to fetch measurements of a workload ran in the past. Also those resources should be active on the cluster. For present cases, please refer to the alternate options in this tool.</p>"},{"location":"cli/#check-alerts","title":"Check alerts","text":"<p>This subcommand can be used to evaluate alerts configured in the given alert profile. Similar to <code>index</code>, the time range is given by the <code>start</code> and <code>end</code> flags.</p>"},{"location":"cli/#destroy","title":"Destroy","text":"<p>This subcommand requires the <code>uuid</code> flag to destroy all namespaces labeled with <code>kube-burner-uuid=&lt;UUID&gt;</code>.</p>"},{"location":"cli/#completion","title":"Completion","text":"<p>Generates bash a completion script that can be imported with: <code>. &lt;(kube-burner completion)</code></p> <p>Or permanently imported with: <code>kube-burner completion &gt; /etc/bash_completion.d/kube-burner</code></p> <p>Note</p> <p>the <code>bash-completion</code> utils must be installed for the kube-burner completion script to work.</p>"},{"location":"measurements/","title":"Measurements","text":"<p>Kube-burner allows you to get further metrics using other mechanisms or data sources, such as the Kubernetes API. These mechanisms are called measurements.</p> <p>Measurements are enabled in the measurements section of the configuration file. This section contains a list of measurements with their options. 'kube-burner' supports the following measurements so far:</p> <p>Warning</p> <p><code>podLatency</code>, as any other measurement, is only captured during a benchmark runtime. It does not work with the <code>index</code> subcommand of kube-burner</p>"},{"location":"measurements/#pod-latency","title":"Pod latency","text":"<p>Collects latencies from the different pod startup phases, these latency metrics are in ms. It can be enabled with:</p> <pre><code>  measurements:\n  - name: podLatency\n</code></pre> <p>This measurement sends its metrics to a configured indexer. The metrics collected are pod latency histograms (<code>podLatencyMeasurement</code>) and four documents holding a summary with different pod latency quantiles of each pod condition (<code>podLatencyQuantilesMeasurement</code>). It is possible to skip indexing the <code>podLatencyMeasurement</code> metric by configuring the field <code>podLatencyMetrics</code> of this measurement to <code>quantiles</code>.</p> <p>One document, such as the following, is indexed per each pod created by the workload that enters in <code>Running</code> condition during the workload:</p> <pre><code>{\n  \"timestamp\": \"2020-11-15T20:28:59.598727718Z\",\n  \"schedulingLatency\": 4,\n  \"initializedLatency\": 20,\n  \"containersReadyLatency\": 2997,\n  \"podReadyLatency\": 2997,\n  \"metricName\": \"podLatencyMeasurement\",\n  \"jobName\": \"kubelet-density\",\n  \"uuid\": \"c40b4346-7af7-4c63-9ab4-aae7ccdd0616\",\n  \"namespace\": \"kubelet-density\",\n  \"podName\": \"kubelet-density-13\",\n  \"jobConfig\": {},\n  \"nodeName\": \"worker-001\"\n}\n</code></pre> <p>Pod latency quantile sample:</p> <pre><code>{\n  \"quantileName\": \"Ready\",\n  \"uuid\": \"23c0b5fd-c17e-4326-a389-b3aebc774c82\",\n  \"P99\": 3774,\n  \"P95\": 3510,\n  \"P50\": 2897,\n  \"max\": 3774,\n  \"avg\": 2876.3,\n  \"timestamp\": \"2020-11-15T22:26:51.553221077+01:00\",\n  \"metricName\": \"podLatencyQuantilesMeasurement\",\n  \"jobConfig\": {}\n},\n{\n  \"quantileName\": \"PodScheduled\",\n  \"uuid\": \"23c0b5fd-c17e-4326-a389-b3aebc774c82\",\n  \"P99\": 64,\n  \"P95\": 8,\n  \"P50\": 5,\n  \"max\": 64,\n  \"avg\": 5.38,\n  \"timestamp\": \"2020-11-15T22:26:51.553225151+01:00\",\n  \"metricName\": \"podLatencyQuantilesMeasurement\",\n  \"jobConfig\": {}\n}\n</code></pre> <p>Where <code>quantileName</code> matches with the pod conditions and can be:</p> <ul> <li><code>PodScheduled</code>: Pod has been scheduled in to a node.</li> <li><code>Initialized</code>: All init containers in the pod have started successfully</li> <li><code>ContainersReady</code>: Indicates whether all containers in the pod are ready.</li> <li><code>Ready</code>: The pod is able to service requests and should be added to the load balancing pools of all matching services.</li> </ul> <p>Note</p> <p>We also log the errorRate of the latencies for user's understanding. It indicates the percentage of pods out of all pods in the workload that got errored during the latency calculations. Currently the threshold for the errorRate is 10% and we do not log latencies if the error is &gt; 10% which indicates a problem with environment.(i.e system under test)</p> <p>Info</p> <p>More information about the pod conditions can be found at the kubernetes documentation site.</p> <p>And the metrics are:</p> <ul> <li><code>P99</code>: 99<sup>th</sup> percentile of the pod condition.</li> <li><code>P95</code>: 95<sup>th</sup> percentile of the pod condition.</li> <li><code>P50</code>: 50<sup>th</sup> percentile of the pod condition.</li> <li><code>Max</code>: Maximum value of the condition.</li> <li><code>Avg</code>: Average value of the condition.</li> </ul>"},{"location":"measurements/#pod-latency-thresholds","title":"Pod latency thresholds","text":"<p>It is possible to establish pod latency thresholds to the different pod conditions and metrics by defining the option <code>thresholds</code> within this measurement:</p> <p>Establishing a threshold of 2000ms in the P99 metric of the <code>Ready</code> condition.</p> <pre><code>  measurements:\n  - name: podLatency\n    thresholds:\n    - conditionType: Ready\n      metric: P99\n      threshold: 2000ms\n</code></pre> <p>Latency thresholds are evaluated at the end of each job, showing an informative message like the following:</p> <pre><code>INFO[2020-12-15 12:37:08] Evaluating latency thresholds\nWARN[2020-12-15 12:37:08] P99 Ready latency (2929ms) higher than configured threshold: 2000ms\n</code></pre> <p>In case of not meeting any of the configured thresholds, like the example above, kube-burner return code will be 1.</p>"},{"location":"measurements/#measure-subcommand-cli-example","title":"Measure subcommand CLI example","text":"<p>Measure subcommand example with relevant options. It is used to fetch measurements on top of resources that were a part of workload ran in past. <pre><code>kube-burner measure --uuid=vchalla --namespaces=cluster-density-v2-0,cluster-density-v2-1,cluster-density-v2-2,cluster-density-v2-3,cluster-density-v2-4 --selector=kube-burner-job=cluster-density-v2 \ntime=\"2023-11-19 17:46:05\" level=info msg=\"\ud83d\udcc1 Creating indexer: elastic\" file=\"kube-burner.go:226\"\ntime=\"2023-11-19 17:46:05\" level=info msg=\"map[kube-burner-job:cluster-density-v2]\" file=\"kube-burner.go:247\"\ntime=\"2023-11-19 17:46:05\" level=info msg=\"\ud83d\udcc8 Registered measurement: podLatency\" file=\"factory.go:85\"\ntime=\"2023-11-19 17:46:06\" level=info msg=\"Stopping measurement: podLatency\" file=\"factory.go:118\"\ntime=\"2023-11-19 17:46:06\" level=info msg=\"Evaluating latency thresholds\" file=\"metrics.go:60\"\ntime=\"2023-11-19 17:46:06\" level=info msg=\"Indexing pod latency data for job: kube-burner-measure\" file=\"pod_latency.go:245\"\ntime=\"2023-11-19 17:46:07\" level=info msg=\"Indexing finished in 417ms: created=4\" file=\"pod_latency.go:262\"\ntime=\"2023-11-19 17:46:08\" level=info msg=\"Indexing finished in 1.32s: created=50\" file=\"pod_latency.go:262\"\ntime=\"2023-11-19 17:46:08\" level=info msg=\"kube-burner-measure: PodScheduled 50th: 0 99th: 0 max: 0 avg: 0\" file=\"pod_latency.go:233\"\ntime=\"2023-11-19 17:46:08\" level=info msg=\"kube-burner-measure: ContainersReady 50th: 9000 99th: 18000 max: 18000 avg: 10680\" file=\"pod_latency.go:233\"\ntime=\"2023-11-19 17:46:08\" level=info msg=\"kube-burner-measure: Initialized 50th: 0 99th: 0 max: 0 avg: 0\" file=\"pod_latency.go:233\"\ntime=\"2023-11-19 17:46:08\" level=info msg=\"kube-burner-measure: Ready 50th: 9000 99th: 18000 max: 18000 avg: 10680\" file=\"pod_latency.go:233\"\ntime=\"2023-11-19 17:46:08\" level=info msg=\"Pod latencies error rate was: 0.00\" file=\"pod_latency.go:236\"\ntime=\"2023-11-19 17:46:08\" level=info msg=\"\ud83d\udc4b Exiting kube-burner vchalla\" file=\"kube-burner.go:209\"\n</code></pre></p>"},{"location":"measurements/#pprof-collection","title":"pprof collection","text":"<p>This measurement can be used to collect Golang profiling information from processes running in pods from the cluster. To do so, kube-burner connects to pods labeled with <code>labelSelector</code> and running in <code>namespace</code>. This measurement uses an implementation similar to <code>kubectl exec</code>, and as soon as it connects to one pod it executes the command <code>curl &lt;pprofURL&gt;</code> to get the pprof data. pprof files are collected in a regular basis configured by the parameter <code>pprofInterval</code>, the collected pprof files are downloaded from the pods to the local directory configured by the parameter <code>pprofDirectory</code> which by default is <code>pprof</code>.</p> <p>As some components require authentication to get profiling information, <code>kube-burner</code> provides two different modalities to address it:</p> <ul> <li>Bearer token authentication: This modality is configured by the variable <code>bearerToken</code>, which holds a valid Bearer token that will be used by cURL to get pprof data. This method is usually valid with kube-apiserver and kube-controller-managers components</li> <li>Certificate Authentication: Usually valid for etcd, this method can be configured using a combination of cert/privKey files or directly using the cert/privkey content, it can be tweaked with the following variables:<ul> <li><code>cert</code>: Base64 encoded certificate.</li> <li><code>key</code>: Base64 encoded private key.</li> <li><code>certFile</code>: Path to a certificate file.</li> <li><code>keyFile</code>: Path to a private key file.</li> </ul> </li> </ul> <p>Note</p> <p>The decoded content of the certificate and private key is written to the files /tmp/pprof.crt and /tmp/pprof.key of the remote pods respectively</p> <p>An example of how to configure this measurement to collect pprof HEAP and CPU profiling data from kube-apiserver and etcd is shown below:</p> <pre><code>  measurements:\n  - name: pprof\n    pprofInterval: 30m\n    pprofDirectory: pprof-data\n    pprofTargets:\n    - name: kube-apiserver-heap\n      namespace: \"openshift-kube-apiserver\"\n      labelSelector: {app: openshift-kube-apiserver}\n      bearerToken: thisIsNotAValidToken\n      url: https://localhost:6443/debug/pprof/heap\n\n    - name: etcd-heap\n      namespace: \"openshift-etcd\"\n      labelSelector: {app: etcd}\n      certFile: etcd-peer-pert.crt\n      keyFile: etcd-peer-pert.key\n      url: https://localhost:2379/debug/pprof/heap\n</code></pre> <p>Warning</p> <p>As mentioned before, this measurement requires the <code>curl</code> command to be available in the target pods.</p>"},{"location":"ocp/","title":"OpenShift Wrapper","text":"<p>The kube-burner binary brings a very opinionated OpenShift wrapper designed to simplify the execution of different workloads in this Kubernetes distribution. This wrapper is hosted under the <code>kube-burner ocp</code> subcommand that currently looks like:</p> <pre><code>$ kube-burner ocp help\nThis subcommand is meant to be used against OpenShift clusters and serve as a shortcut to trigger well-known workloads\n\nUsage:\n  kube-burner ocp [command]\n\nAvailable Commands:\n  cluster-density-ms             Runs cluster-density-ms workload\n  cluster-density-v2             Runs cluster-density-v2 workload\n  crd-scale                      Runs crd-scale workload\n  index                          Runs index sub-command\n  networkpolicy-matchexpressions Runs networkpolicy-matchexpressions workload\n  networkpolicy-matchlabels      Runs networkpolicy-matchlabels workload\n  networkpolicy-multitenant      Runs networkpolicy-multitenant workload\n  node-density                   Runs node-density workload\n  node-density-cni               Runs node-density-cni workload\n  node-density-heavy             Runs node-density-heavy workload\n  pvc-density                    Runs pvc-density workload\n  web-burner-cluster-density     Runs web-burner-cluster-density workload\n  web-burner-init                Runs web-burner-init workload\n  web-burner-node-density        Runs web-burner-node-density workload\n\nFlags:\n      --alerting                  Enable alerting (default true)\n      --burst int                 Burst (default 20)\n      --es-index string           Elastic Search index\n      --es-server string          Elastic Search endpoint\n      --extract                   Extract workload in the current directory\n      --gc                        Garbage collect created namespaces (default true)\n      --gc-metrics                Collect metrics during garbage collection\n  -h, --help                      help for ocp\n      --local-indexing            Enable local indexing\n      --metrics-endpoint string   YAML file with a list of metric endpoints\n      --profile-type string       Metrics profile to use, supported options are: regular, reporting or both (default \"both\")\n      --qps int                   QPS (default 20)\n      --timeout duration          Benchmark timeout (default 4h0m0s)\n      --user-metadata string      User provided metadata file, in YAML format\n      --uuid string               Benchmark UUID (default \"9e14107b-3d15-4904-843b-eac6d23ebd40\")\n\n\nGlobal Flags:\n      --log-level string   Allowed values: debug, info, warn, error, fatal (default \"info\")\n\nUse \"kube-burner ocp [command] --help\" for more information about a command.\n</code></pre>"},{"location":"ocp/#usage","title":"Usage","text":"<p>Some of the benefits the OCP wrapper provides are:</p> <ul> <li>Simplified execution of the supported workloads. (Only some flags are required)</li> <li>Indexes OpenShift metadata along with the Benchmark result. This document can be found with the following query: <code>uuid: &lt;benchmkark-uuid&gt; AND metricName.keyword: \"clusterMetadata\"</code></li> <li>Prevents modifying configuration files to tweak some of the parameters of the workloads.</li> <li>Discovers the Prometheus URL and authentication token, so the user does not have to perform those operations before using them.</li> </ul> <p>In order to trigger one of the supported workloads using this subcommand, you must run kube-burner using the subcommand <code>ocp</code>. The workloads are embedded in the kube-burner binary:</p> <p>Running node-density with 100 pods per node</p> <pre><code>kube-burner ocp node-density --pods-per-node=100\n</code></pre> <p>With the command above, the wrapper will calculate the required number of pods to deploy across all worker nodes of the cluster.</p>"},{"location":"ocp/#multiple-endpoints-support","title":"Multiple endpoints support","text":"<p>The flag <code>--metrics-endpoint</code> can be used to interact with multiple Prometheus endpoints For example:</p> <pre><code>kube-burner ocp cluster-density-v2 --iterations=1 --churn-duration=2m0s --es-index kube-burner --es-server https://www.esurl.com:443 --metrics-endpoint metrics-endpoints.yaml\n</code></pre>"},{"location":"ocp/#cluster-density-workloads","title":"Cluster density workloads","text":"<p>This workload family is a control-plane density focused workload that that creates different objects across the cluster. There are 2 different variants cluster-density-v2 and cluster-density-ms.</p> <p>Each iteration of these create a new namespace, the three support similar configuration flags. Check them out from the subcommand help.</p> <p>Info</p> <p>Workload churning of 1h is enabled by default in the <code>cluster-density</code> workloads; you can disable it by passing <code>--churn=false</code> to the workload subcommand.</p>"},{"location":"ocp/#cluster-density-v2","title":"cluster-density-v2","text":"<p>Each iteration creates the following objects in each of the created namespaces:</p> <ul> <li>1 image stream.</li> <li>1 build. The OCP internal container registry must be set-up previously because the resulting container image will be pushed there.</li> <li>3 deployments with two pod 2 replicas (nginx) mounting 4 secrets, 4 config maps, and 1 downward API volume each.</li> <li>2 deployments with two pod 2 replicas (curl) mounting 4 Secrets, 4 config maps and 1 downward API volume each. These pods have configured a readiness probe that makes a request to one of the services and one of the routes created by this workload every 10 seconds.</li> <li>5 services, each one pointing to the TCP/8080 port of one of the nginx deployments.</li> <li>2 edge routes pointing to the to first and second services respectively.</li> <li>10 secrets containing a 2048-character random string.</li> <li>10 config maps containing a 2048-character random string.</li> <li>3 network policies:<ul> <li>deny-all traffic</li> <li>allow traffic from client/nginx pods to server/nginx pods</li> <li>allow traffic from openshift-ingress namespace (where routers are deployed by default) to the namespace</li> </ul> </li> </ul>"},{"location":"ocp/#cluster-density-ms","title":"cluster-density-ms","text":"<p>Lightest version of this workload family, each iteration the following objects in each of the created namespaces:</p> <ul> <li>1 image stream.</li> <li>4 deployments with two pod replicas (pause) mounting 4 secrets, 4 config maps, and 1 downward API volume each.</li> <li>2 services, each one pointing to the TCP/8080 and TCP/8443 ports of the first and second deployment respectively.</li> <li>1 edge route pointing to the to first service.</li> <li>20 secrets containing a 2048-character random string.</li> <li>10 config maps containing a 2048-character random string.</li> </ul>"},{"location":"ocp/#node-density-workloads","title":"Node density workloads","text":"<p>The workloads of this family create a single namespace with a set of pods, deployments, and services depending on the workload.</p>"},{"location":"ocp/#node-density","title":"node-density","text":"<p>This workload is meant to fill with pause pods all the worker nodes from the cluster. It can be customized with the following flags. This workload is usually used to measure the Pod's ready latency KPI.</p>"},{"location":"ocp/#node-density-cni","title":"node-density-cni","text":"<p>It creates two deployments, a client/curl and a server/nxing, and 1 service backed by the previous server pods. The client application has configured an startup probe that makes requests to the previous service every second with a timeout of 600s.</p> <p>Note: This workload calculates the number of iterations to create from the number of nodes and desired pods per node.  In order to keep the test scalable and performant, chunks of 1000 iterations will by broken into separate namespaces, using the config variable <code>iterationsPerNamespace</code>.</p>"},{"location":"ocp/#node-density-heavy","title":"node-density-heavy","text":"<p>Creates two deployments, a postgresql database, and a simple client that performs periodic insert queries (configured through liveness and readiness probes) on the previous database and a service that is used by the client to reach the database.</p> <p>Note: this workload calculates the number of iterations to create from the number of nodes and desired pods per node.  In order to keep the test scalable and performant, chunks of 1000 iterations will by broken into separate namespaces, using the config variable <code>iterationsPerNamespace</code>.</p>"},{"location":"ocp/#network-policy-workloads","title":"Network Policy workloads","text":"<p>With the help of networkpolicy object we can control traffic flow at the IP address or port level in Kubernetes. A networkpolicy can come in various shapes and sizes. Allow traffic from a specific namespace, Deny traffic from a specific pod IP, Deny all traffic, etc. Hence we have come up with a few test cases which try to cover most of them. They are as follows.</p>"},{"location":"ocp/#networkpolicy-multitenant","title":"networkpolicy-multitenant","text":"<ul> <li>500 namespaces</li> <li>20 pods in each namespace. Each pod acts as a server and a client</li> <li>Default deny networkpolicy is applied first that blocks traffic to any test namespace</li> <li>3 network policies in each namespace that allows traffic from the same namespace and two other namespaces using namespace selectors</li> </ul>"},{"location":"ocp/#networkpolicy-matchlabels","title":"networkpolicy-matchlabels","text":"<ul> <li>5 namespaces</li> <li>100 pods in each namespace. Each pod acts as a server and a client</li> <li>Each pod with 2 labels and each label shared is by 5 pods</li> <li>Default deny networkpolicy is applied first</li> <li>Then for each unique label in a namespace we have a networkpolicy with that label as a podSelector which allows traffic from pods with some other randomly selected label. This translates to 40 networkpolicies/namespace</li> </ul>"},{"location":"ocp/#networkpolicy-matchexpressions","title":"networkpolicy-matchexpressions","text":"<ul> <li>5 namespaces</li> <li>25 pods in each namespace. Each pod acts as a server and a client</li> <li>Each pod with 2 labels and each label shared is by 5 pods</li> <li>Default deny networkpolicy is applied first</li> <li>Then for each unique label in a namespace we have a networkpolicy with that label as a podSelector which allows traffic from pods which don't have some other randomly-selected label. This translates to 10 networkpolicies/namespace</li> </ul>"},{"location":"ocp/#web-burner-workloads","title":"Web-burner workloads","text":"<p>This workload is meant to emulate some telco specific workloads. Before running web-burner-node-density or web-burner-cluster-density load the environment with web-burner-init first (without the garbage collection flag: <code>--gc=false</code>).</p> <p>Pre-requisites:  - At least two worker nodes  - At least one of the worker nodes must have the <code>node-role.kubernetes.io/worker-spk</code> label</p>"},{"location":"ocp/#web-burner-init","title":"web-burner-init","text":"<ul> <li>35 (macvlan/sriov) networks for 35 lb namespace</li> <li>35 lb-ns</li> <li>1 frr config map, 4 emulated lb pods on each namespace</li> <li>35 app-ns<ul> <li>1 emulated lb pod on each namespace for bfd session</li> </ul> </li> </ul>"},{"location":"ocp/#web-burner-node-density","title":"web-burner-node-density","text":"<ul> <li>35 app-ns</li> <li>3 app pods and services on each namespace</li> <li>35 normal-ns<ul> <li>1 service with 60 normal pod endpoints on each namespace</li> </ul> </li> </ul>"},{"location":"ocp/#web-burner-cluster-density","title":"web-burner-cluster-density","text":"<ul> <li>20 normal-ns<ul> <li>30 configmaps, 38 secrets, 38 normal pods and services, 5 deployments with 2 replica pods on each namespace</li> </ul> </li> <li>35 served-ns</li> <li>3 app pods on each namespace</li> <li>2 app-served-ns<ul> <li>1 service(15 ports) with 84 pod endpoints, 1 service(15 ports) with 56 pod endpoints, 1 service(15 ports) with 25 pod endpoints</li> <li>3 service(15 ports each) with 24 pod endpoints, 3 service(15 ports each) with 14 pod endpoints</li> <li>6 service(15 ports each) with 12 pod endpoints, 6 service(15 ports each) with 10 pod endpoints, 6 service(15 ports each) with 9 pod endpoints</li> <li>12 service(15 ports each) with 8 pod endpoints, 12 service(15 ports each) with 6 pod endpoints, 12 service(15 ports each) with 5 pod endpoints</li> <li>29 service(15 ports each) with 4 pod endpoints, 29 service(15 ports each) with 6 pod endpoints</li> </ul> </li> </ul>"},{"location":"ocp/#index","title":"Index","text":"<p>Just like the traditional kube-burner, ocp wrapper also has an indexing functionality which is exposed as <code>index</code> subcommand.</p> <pre><code>$ kube-burner ocp index --help\nIf no other indexer is specified, local indexer is used by default\n\nUsage:\n  kube-burner ocp index [flags]\n\nFlags:\n  -m, --metrics-profile string     Metrics profile file (default \"metrics.yml\")\n      --metrics-directory string   Directory to dump the metrics files in, when using default local indexing (default \"collected-metrics\")\n  -s, --step duration              Prometheus step size (default 30s)\n      --start int                  Epoch start time\n      --end int                    Epoch end time\n  -j, --job-name string            Indexing job name (default \"kube-burner-ocp-indexing\")\n      --user-metadata string       User provided metadata file, in YAML format\n  -h, --help                       help for index\n</code></pre> <p>Please refer to indexing section for better understanding on the functionality.</p>"},{"location":"ocp/#metrics-profile-type","title":"Metrics-profile type","text":"<p>By specifying <code>--profile-type</code>, kube-burner can use two different metrics profiles when scraping metrics from prometheus. By default is configured with <code>both</code>, meaning that it will use the regular metrics profiles bound to the workload in question and the reporting metrics profile.</p> <p>When using the regular profiles (metrics-aggregated or metrics), kube-burner scrapes and indexes metrics timeseries.</p> <p>The reporting profile is very useful to reduce the number of documents sent to the configured indexer. Thanks to the combination of aggregations and instant queries for prometheus metrics, and 4 summaries for latency measurements, only a few documents will be indexed per benchmark. This flag makes possible to specify one or both of these profiles indistinctly.</p>"},{"location":"ocp/#customizing-workloads","title":"Customizing workloads","text":"<p>It is possible to customize any of the above workload configurations by extracting, updating, and finally running it:</p> <pre><code>$ kube-burner ocp node-density --extract\n$ ls\nalerts.yml  metrics.yml  node-density.yml  pod.yml  metrics-report.yml\n$ vi node-density.yml                               # Perform modifications accordingly\n$ kube-burner ocp node-density --pods-per-node=100  # Run workload\n</code></pre>"},{"location":"ocp/#cluster-metadata","title":"Cluster metadata","text":"<p>When the benchmark finishes, kube-burner will index the cluster metadata in the configured indexer. Currently. this is based on the following Golang struct:</p> <pre><code>type BenchmarkMetadata struct {\n  ocpmetadata.ClusterMetadata\n  UUID         string                 `json:\"uuid\"`\n  Benchmark    string                 `json:\"benchmark\"`\n  Timestamp    time.Time              `json:\"timestamp\"`\n  EndDate      time.Time              `json:\"endDate\"`\n  Passed       bool                   `json:\"passed\"`\n  UserMetadata map[string]interface{} `json:\"metadata,omitempty\"`\n}\n</code></pre> <p>Where <code>ocpmetadata.ClusterMetadata</code> is an embed struct inherited from the go-commons library, which has the following fields:</p> <pre><code>// Type to store cluster metadata\ntype ClusterMetadata struct {\n  MetricName       string `json:\"metricName,omitempty\"`\n  Platform         string `json:\"platform\"`\n  OCPVersion       string `json:\"ocpVersion\"`\n  OCPMajorVersion  string `json:\"ocpMajorVersion\"`\n  K8SVersion       string `json:\"k8sVersion\"`\n  MasterNodesType  string `json:\"masterNodesType\"`\n  WorkerNodesType  string `json:\"workerNodesType\"`\n  MasterNodesCount int    `json:\"masterNodesCount\"`\n  InfraNodesType   string `json:\"infraNodesType\"`\n  WorkerNodesCount int    `json:\"workerNodesCount\"`\n  InfraNodesCount  int    `json:\"infraNodesCount\"`\n  TotalNodes       int    `json:\"totalNodes\"`\n  SDNType          string `json:\"sdnType\"`\n  ClusterName      string `json:\"clusterName\"`\n  Region           string `json:\"region\"`\n  ExecutionErrors  string `json:\"executionErrors\"`\n}\n</code></pre> <p>MetricName is hardcoded to <code>clusterMetadata</code></p> <p>Info</p> <p>It's important to note that every document indexed when using an OCP wrapper workload will include an small subset of the previous fields: <pre><code>platform\nocpVersion\nocpMajorVersion\nk8sVersion\ntotalNodes\nsdnType\n</code></pre></p>"},{"location":"contributing/","title":"Contributing to kube-burner","text":"<p>If you want to contribute to kube-burner, you can do so by submitting a Pull Request, Issue or starting a Discussion.</p>"},{"location":"contributing/#making-changes-and-opening-a-pull-request","title":"Making changes and opening a Pull Request","text":"<p>For submitting a change upstream, please fork the repository and clone your forked repository. <pre><code>$ git clone http://github.com/YOUR-USERNAME/kube-burner\n$ cd kube-burner\n$ git checkout -b &lt;branch_name&gt;\n$ make lint\n$ git add &lt;changes&gt;\n$ git commit -m -s \"&lt;Details of the Commit&gt;\"\n$ git push\n</code></pre></p>"},{"location":"contributing/#ci-and-linting","title":"CI and Linting","text":"<p>For running pre-commit checks on your code before comitting code and opening a PR, you can use the <code>pre-commit run</code> functionality.  See CI docs for more information on running pre-commits.</p>"},{"location":"contributing/#building","title":"Building","text":"<p>To build kube-burner just execute <code>make build</code>, once finished the kube-burner binary should be available at <code>./bin/&lt;arch&gt;/kube-burner</code>.</p> <p>Note</p> <p>Building kube-burner requires <code>golang &gt;=1.19</code></p> <pre><code>$ make build\nbuilding kube-burner 0.1.0\nGOPATH=/home/kube-burner/go\nCGO_ENABLED=0 go build -v -ldflags \"-X github.com/kube-burner/kube-burner/version.GitCommit=d91c8cc35cb458a4b80a5050704a51c7c6e35076 -X github.com/kube-burner/kube-burner/version.BuildDate=2020-08-19-19:10:09 -X github.com/kube-burner/kube-burner/version.GitBranch=master\" -o bin/kube-burner\n</code></pre>"},{"location":"contributing/pullrequest/","title":"Pullrequest","text":"<p>The pull Request Workflow, defined in the <code>pullrequest.yml</code> file, is triggered on <code>pull_request_target</code> events to the branches <code>master</code> and <code>main</code>. It has three jobs: linters, build, tests, and report_results.</p> <pre><code>graph LR\n  A[pull_request_target] --&gt; B[linters];\n  B --&gt; C[build];\n  C --&gt; D[tests];\n  C --&gt; D[report_results];</code></pre>"},{"location":"contributing/pullrequest/#linters","title":"Linters","text":"<p>This job performs the following steps:</p> <ol> <li>Checks out the code</li> <li>Installs pre-commit.</li> <li>Runs pre-commit hooks to execute code linting based on <code>.pre-commit-config.yaml</code> file</li> </ol>"},{"location":"contributing/pullrequest/#running-local-pre-commit","title":"Running local pre-commit","text":"<p>Info</p> <p>Main purpose for pre-commit is to allow developers to pass the Lint Checks before commiting the code. Same checks will be executed on all the commits once they are pushed to GitHub</p> <p>To install pre-commit checks locally, follow these steps:</p> <ol> <li> <p>Install pre-commit by running the following command:</p> <pre><code>pip install pre-commit\n</code></pre> </li> <li> <p><code>ruby</code> is required for running the Markdown Linter, installation will depends on your Operating System, for example, on Fedora:</p> <pre><code>dnf install -y ruby\n</code></pre> </li> <li> <p>Initialize pre-commit on the repo:</p> <pre><code>pre-commit install\n</code></pre> </li> </ol> <p>To run pre-commit manually for all files, you can use <code>make lint</code></p> <pre><code>make lint\n</code></pre> <p>Or you can run against an especific file:</p> <pre><code>$ pre-commit run --files README.md\ngolangci-lint........................................(no files to check)Skipped\nMarkdownlint.............................................................Passed\n</code></pre> <pre><code>$ pre-commit run --files ./cmd/kube-burner/kube-burner.go\ngolangci-lint............................................................Passed\nMarkdownlint.........................................(no files to check)Skipped\n</code></pre> <pre><code>$ pre-commit run --all-files\ngolangci-lint............................................................Passed\nMarkdownlint.............................................................Passed\n</code></pre> <p>Pre-commit hooks can be updated using <code>pre-commit autoupdate</code>:</p> <pre><code>$ pre-commit autoupdate\n[WARNING] The 'rev' field of repo 'https://github.com/golangci/golangci-lint' appears to be a mutable reference (moving tag / branch).  Mutable references are never updated after first install and are not supported.  See https://pre-commit.com/#using-the-latest-version-for-a-repository for more details.  Hint: `pre-commit autoupdate` often fixes this.\n[WARNING] The 'rev' field of repo 'https://github.com/markdownlint/markdownlint' appears to be a mutable reference (moving tag / branch).  Mutable references are never updated after first install and are not supported.  See https://pre-commit.com/#using-the-latest-version-for-a-repository for more details.  Hint: `pre-commit autoupdate` often fixes this.\n[WARNING] The 'rev' field of repo 'https://github.com/jumanjihouse/pre-commit-hooks' appears to be a mutable reference (moving tag / branch).  Mutable references are never updated after first install and are not supported.  See https://pre-commit.com/#using-the-latest-version-for-a-repository for more details.  Hint: `pre-commit autoupdate` often fixes this.\n[https://github.com/golangci/golangci-lint] updating master -&gt; v1.52.2\n[https://github.com/markdownlint/markdownlint] updating master -&gt; v0.12.0\n[https://github.com/jumanjihouse/pre-commit-hooks] updating master -&gt; 3.0.0\n</code></pre>"},{"location":"contributing/pullrequest/#build","title":"Build","text":"<p>The \"build\" job uses the file <code>builders.yml</code> file to build binaries and images, performing the following steps:</p> <ol> <li>Sets up Go 1.19.</li> <li>Checks out the code.</li> <li>Builds the code using the <code>make build</code> command.</li> <li>Builds container images using the <code>make images</code> command</li> <li>Installs the built artifacts using <code>sudo make install</code> command.</li> <li>Uploads the built binary file as an artifact named kube-burner.</li> </ol>"},{"location":"contributing/pullrequest/#tests","title":"Tests","text":"<p>The Tests Workflow, defined in the <code>tests-k8s.yml</code> file, has two jobs: test-k8s and ocp-wrapper.</p> <p>Tests are orchestrated using BATS</p>"},{"location":"contributing/pullrequest/#test-k8s","title":"Test K8s","text":"<p>For testing kube-burner on a real k8s environment, we use kind, with different K8s versions.</p> <p>It creates a quick k8s deployment using containers with podman.</p> <p>The test-k8s job performs the following steps:</p> <ol> <li>Checks out the code.</li> <li>Downloads the kube-burner binary artifact.</li> <li>Installs Bats and the oc command-line tool.</li> <li>Executes tests using Bats and generates a JUnit report.</li> <li>Uploads the test results artifact.</li> <li>Publishes the test report using the mikepenz/action-junit-report action.</li> </ol>"},{"location":"contributing/pullrequest/#test-ocp","title":"Test OCP","text":"<p>For testing OCP, we use a real OCP</p> <p>The ocp-wrapper job performs similar steps to the \"test-k8s\" job but with additional OpenShift-specific configurations and secrets.</p>"},{"location":"contributing/pullrequest/#kube-burner-tests-executed","title":"Kube-burner tests executed","text":"<p>The kube-burner tests executed on each environment can be found on following files included in the folder <code>/test/</code></p> <ul> <li>K8s: <code>test/test-k8s.bats</code></li> <li>OCP: <code>test/test-ocp.bats</code></li> </ul>"},{"location":"contributing/pullrequest/#report-results","title":"Report Results","text":"<p>The \"report_results\" job performs the following steps:</p> <ol> <li>Checks out the code.</li> <li>Downloads the Kubernetes and OpenShift test results artifacts.</li> <li>Publishes the test results and add a comment in the PR</li> </ol>"},{"location":"contributing/release/","title":"Release","text":"<p>The Release workflow, defined in the <code>release.yml</code> file, when a new tag is pushed it triggers: release-build and image-upload.</p> <pre><code>graph LR\n  A[new tag pushed] --&gt; B[release_build];\n  A --&gt; C[image-upload];\n  B --&gt; D[docs-update];</code></pre> <p>The <code>docs-update</code> job is triggered when a new release is <code>published</code>, this is done by the <code>release_build</code> job.</p>"},{"location":"contributing/release/#release-build","title":"Release Build","text":"<p>This job uses the GoRelease workflow defined in the <code>gorelease.yml</code> file to create a new release of the project performing the following steps:</p> <ol> <li>Checks out the code into the Go module directory.</li> <li>Sets up Go 1.19.</li> <li>Runs GoReleaser to create a new release, including the removal of previous distribution files.</li> </ol>"},{"location":"contributing/release/#image-upload","title":"Image Upload","text":"<p>This job uses the Upload Containers to Quay workflow defined in the <code>image-upload.yml</code> file to upload containers to the Quay registry for multiple architectures (arm64, amd64, ppc64le, s390x) performing the following steps:</p> <ol> <li>Installs the dependencies required for multi-architecture builds.</li> <li>Checks out the code.</li> <li>Sets up Go 1.19.</li> <li>Logs in to Quay using the provided QUAY_USER and QUAY_TOKEN secrets.</li> <li>Builds the kube-burner binary for the specified architecture.</li> <li>Builds the container image using the make images command, with environment variables for architecture and organization.</li> <li>Pushes the container image to Quay using the make push command.</li> </ol> <p>The \"manifest\" job builds a container manifest and runs after the \"containers\" job. It performs the following steps:</p> <ol> <li>Checks out the code.</li> <li>Logs in to Quay using the provided QUAY_USER and QUAY_TOKEN secrets.</li> <li>Creates and pushes the container manifest using the make manifest command, with the organization specified.</li> </ol>"},{"location":"contributing/release/#docs-update","title":"Docs Update","text":"<p>Uses the <code>Deploy docs</code> workflow defined in the <code>docs.yml</code> file to generate and deploy the documentation performing the following steps:</p> <ol> <li>Checks out the code.</li> <li>Sets up Python 3.x.</li> <li>Exports the release tag version as an environment variable.</li> <li>Sets up the Git configuration for documentation deployment.</li> <li>Installs the required dependencies, including mkdocs-material and mike.</li> <li>Deploys the documentation using the mike deploy command, with specific parameters for updating aliases and including the release tag version in the deployment message.</li> </ol>"},{"location":"contributing/tests/","title":"Tests","text":"<p>The Tests Workflow, defined in the <code>tests-k8s.yml</code> file, has two jobs: test-k8s and ocp-wrapper.</p> <p>Tests are orchestrated using BATS</p>"},{"location":"contributing/tests/#test-k8s","title":"Test K8s","text":"<p>For testing kube-burner on a real k8s environment, we use kind, with different K8s versions.</p> <p>It creates a quick k8s deployment using containers with podman.</p> <p>The test-k8s job performs the following steps:</p> <ol> <li>Checks out the code.</li> <li>Downloads the kube-burner binary artifact.</li> <li>Installs Bats and the oc command-line tool.</li> <li>Executes tests using Bats and generates a JUnit report.</li> <li>Uploads the test results artifact.</li> <li>Publishes the test report using the mikepenz/action-junit-report action.</li> </ol>"},{"location":"contributing/tests/#test-ocp","title":"Test OCP","text":"<p>For testing OCP, we use a real OCP</p> <p>The ocp-wrapper job performs similar steps to the \"test-k8s\" job but with additional OpenShift-specific configurations and secrets.</p>"},{"location":"contributing/tests/#kube-burner-tests-executed","title":"Kube-burner tests executed","text":"<p>The kube-burner tests executed on each environment can be found on following files included in the folder <code>/test/</code></p> <ul> <li>K8s: <code>test/test-k8s.bats</code></li> <li>OCP: <code>test/test-ocp.bats</code></li> </ul>"},{"location":"observability/","title":"Overview","text":"<p>Performing a benchmark using kube-burner is relatively simple. However, it is sometimes necessary to analyze and be able to react to some KPIs in order to validate a benchmark. That  is why kube-burner ships  metric-collection and alerting systems based on Prometheus expressions.</p> <p>Kube-burner also ships an indexing feature that, in combination with the metric-collection and alerting features, can be used to analyze these KPIs in an external tool, such as Grafana or similar.</p> <p>The benchmark stages include the following:</p> <pre><code>flowchart TD\n    A[/\"Init benchmark\"/] -- Read config --&gt; B(Start measurements)\n    B --&gt; C(Run Job)\n    C --&gt; D{Next job?}\n    D --&gt; |Yes| C\n    D --&gt; |No| E[Stop measurements]\n    E --&gt; F[Evaluate alerts]\n    F --&gt; G[(Index results)]\n    G --&gt; H[Indexing]\n    H --&gt; I*[/End/]</code></pre>"},{"location":"observability/alerting/","title":"Alerting","text":"<p>Kube-burner includes an alerting feature able to evaluate Prometheus expressions in order to fire and index alerts.</p>"},{"location":"observability/alerting/#configuration","title":"Configuration","text":"<p>Alerting is configured through a configuration file pointed by the flag <code>--alert-profile</code> or <code>-a</code>, which is a YAML formatted file with the following shape:</p> <pre><code>- expr: avg_over_time(histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[2m]))[5m:]) &gt; 0.01\n  description: 5 minutes avg. etcd fsync latency on {{$labels.pod}} higher than 10ms {{$value}}\n  severity: error\n\n- expr: avg_over_time(histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket[5m]))[5m:]) &gt; 0.1\n  description: 5 minutes avg. etcd netowrk peer round trip on {{$labels.pod}} higher than 100ms {{$value}}\n  severity: error\n\n- expr: increase(etcd_server_leader_changes_seen_total[2m]) &gt; 0\n  description: etcd leader changes observed\n  severity: error\n</code></pre> <p>Where <code>expr</code> holds the PromQL to evaluate and <code>description</code> holds a description of the alert, that will be printed/indexed when the alert fires. In the <code>description</code> field, you can use Prometheus labels to increase alert readability by using the syntax <code>{{$labels.&lt;label_name&gt;}}</code> and also print value of the value that fired the alarm using <code>{{$value}}</code>.</p> <p>You can configure alerts with a severity. Each severity level has different effects. These are:</p> <ul> <li><code>info</code>: Prints an info message with the alarm description to stdout. By default all expressions have this severity.</li> <li><code>warning</code>: Prints a warning message with the alarm description to stdout.</li> <li><code>error</code>: Prints an error message with the alarm description to stdout and makes kube-burner rc = 1</li> <li><code>critical</code>: Prints a fatal message with the alarm description to stdout and aborts execution immediately with rc =1 0</li> </ul>"},{"location":"observability/alerting/#using-the-elapsed-variable","title":"Using the elapsed variable","text":"<p>There is a special go-template variable that can be used within the Prometheus expression, the variable elapsed is set to the value of the job duration (or the range given to check-alerts). This variable is especially useful in expressions using aggregations over time functions. i.e:</p> <pre><code>- expr: avg_over_time(histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[2m]))[{{ .elapsed }}:]) &gt; 0.01\n  description: avg. etcd fsync latency on {{$labels.pod}} higher than 10ms {{$value}}\n  severity: error\n</code></pre>"},{"location":"observability/alerting/#checking-alerts","title":"Checking alerts","text":"<p>It is possible to look for alerts without triggering a kube-burner workload by using the <code>check-alerts</code> subcommand. Similar to the <code>index</code> CLI option, this option accepts the flags <code>--start</code> and <code>--end</code> to evaluate the alerts at a given time range.</p> <pre><code>$ kube-burner check-alerts -u https://prometheus.url.com -t ${token} -a alert-profile.yml\nINFO[2020-12-10 11:47:23] \ud83d\udc7d Initializing prometheus client\nINFO[2020-12-10 11:47:24] \ud83d\udd14 Initializing alert manager\nINFO[2020-12-10 11:47:24] Evaluating expression: 'avg_over_time(histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[2m]))[5m:]) &gt; 0.01'\nERRO[2020-12-10 11:47:24] Alert triggered at 2020-12-10 11:01:53 +0100 CET: '5 minutes avg. etcd fsync latency on etcd-ip-10-0-213-209.us-west-2.compute.internal higher than 10ms 0.010281314285714311'\nINFO[2020-12-10 11:47:24] Evaluating expression: 'avg_over_time(histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket[5m]))[5m:]) &gt; 0.1'\nINFO[2020-12-10 11:47:24] Evaluating expression: 'increase(etcd_server_leader_changes_seen_total[2m]) &gt; 0'\nINFO[2020-12-10 11:47:24] Evaluating expression: 'avg_over_time(histogram_quantile(0.99, sum(apiserver_request_duration_seconds_bucket{apiserver=\"kube-apiserver\",verb=~\"POST|PUT|DELETE|PATCH|CREATE\"}) by (verb,resource,subresource,le))[5m\n:]) &gt; 1'\nINFO[2020-12-10 11:47:25] Evaluating expression: 'avg_over_time(histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{apiserver=\"kube-apiserver\",verb=\"GET\",scope=\"resource\"}[2m])) by (verb,resource,subresource,le))[5\nm:]) &gt; 1'\nINFO[2020-12-10 11:47:25] Evaluating expression: 'avg_over_time(histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{apiserver=\"kube-apiserver\",verb=\"LIST\",scope=\"namespace\"}[2m])) by (verb,resource,subresource,le))\n[5m:]) &gt; 5'\nINFO[2020-12-10 11:47:26] Evaluating expression: 'avg_over_time(histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{apiserver=\"kube-apiserver\",verb=\"LIST\",scope=\"cluster\"}[2m])) by (verb,resource,subresource,le))[5\nm:]) &gt; 30'\n</code></pre>"},{"location":"observability/alerting/#indexing-alerts","title":"Indexing alerts","text":"<p>When indexing is enabled, the alerts sent by kube-burner are automatically indexed by the provided <code>indexer</code>. The documents generated by these alerts have the following structure:</p> <pre><code>{\n  \"timestamp\": \"2023-01-19T22:20:10+01:00\",\n  \"uuid\": \"c0dd0d60-ddf5-488e-bf2f-b8960fc2b5ab\",\n  \"severity\": \"warning\",\n  \"description\": \"5 minutes avg. 99th etcd fsync latency on etcd-ip-10-0-133-30.us-west-2.compute.internal higher than 10ms. 0.004s\",\n  \"metricName\": \"alert\"\n}\n</code></pre>"},{"location":"observability/indexing/","title":"Indexing","text":"<p>Kube-burner can index the collected metrics into a given indexer.</p>"},{"location":"observability/indexing/#indexers","title":"Indexers","text":"<p>Configured in the <code>indexerConfig</code> object, they can be tweaked by the following parameters:</p> Option Description Type Default <code>type</code> Type of indexer String \"\" <p>Note</p> <p>Currently, <code>elastic</code>, <code>opensearch</code> and <code>local</code> are the only supported indexers</p>"},{"location":"observability/indexing/#elasticopensearch","title":"Elastic/OpenSearch","text":"<p>This indexer send collected documents to Elasticsearch 7 instances or OpenSearch instances.</p> <p>The <code>elastic</code> or <code>opensearch</code> indexer can be configured by the parameters below:</p> Option Description Type Default <code>esServers</code> List of Elasticsearch or OpenSearch instances List \"\" <code>defaultIndex</code> Default index to send the Prometheus metrics into String \"\" <code>insecureSkipVerify</code> TLS certificate verification Boolean false <p>OpenSearch is backwards compatible with Elasticsearch and kube-burner does not use any version checks. Therefore, kube-burner with OpenSearch indexing should work as expected.</p> <p>Info</p> <p>It is possible to index documents in an authenticated Elasticsearch or OpenSearch instance using the notation <code>http(s)://[username]:[password]@[address]:[port]</code> in the <code>esServers</code> parameter.</p>"},{"location":"observability/indexing/#local","title":"Local","text":"<p>This indexer writes collected metrics to local files.</p> <p>The <code>local</code> indexer can be configured by the parameters below:</p> Option Description Type Default <code>metricsDirectory</code> Collected metric will be dumped here. String collected-metrics <code>createTarball</code> Create metrics tarball Boolean false <code>tarballName</code> Name of the metrics tarball String kube-burner-metrics.tgz"},{"location":"observability/indexing/#job-summary","title":"Job Summary","text":"<p>When an indexer is configured, a document holding the job summary is indexed at the end of the job. This is useful to identify the parameters the job was executed with. It also contains the timestaps of the execution phase (<code>timestamp</code> and <code>endTimestamp</code>) as well as the cleanup phase (<code>cleanupTimestamp</code> and <code>cleanupEndTimestamp</code>).</p> <p>This document looks like:</p> <pre><code>{\n  \"timestamp\": \"2023-08-29T00:17:27.942960538Z\",\n  \"endTimestamp\": \"2023-08-29T00:18:15.817272025Z\",\n  \"uuid\": \"83bfcb20-54f1-43f4-b2ad-ad04c2f4fd16\",\n  \"metricName\": \"jobSummary\",\n  \"elapsedTime\": 48,\n  \"cleanupTimestamp\": \"2023-08-29T00:18:18.015107794Z\",\n  \"cleanupEndTimestamp\": \"2023-08-29T00:18:49.014541929Z\",\n  \"metricName\": \"jobSummary\",\n  \"elapsedTime\": 8.768932955,\n  \"jobConfig\": {\n    \"jobIterations\": 10,\n    \"jobIterationDelay\": 0,\n    \"jobPause\": 0,\n    \"name\": \"kubelet-density\",\n    \"objects\": [\n      {\n        \"objectTemplate\": \"templates/pod.yml\",\n        \"replicas\": 1,\n        \"inputVars\": {\n          \"containerImage\": \"gcr.io/google_containers/pause-amd64:3.0\"\n        }\n      }\n    ],\n    \"jobType\": \"create\",\n    \"qps\": 5,\n    \"burst\": 5,\n    \"namespace\": \"kubelet-density\",\n    \"waitFor\": null,\n    \"maxWaitTimeout\": 43200000000000,\n    \"waitForDeletion\": true,\n    \"podWait\": false,\n    \"waitWhenFinished\": true,\n    \"cleanup\": true,\n    \"namespacedIterations\": false,\n    \"verifyObjects\": true,\n    \"errorOnVerify\": false\n  }\n}\n</code></pre>"},{"location":"observability/indexing/#metric-exporting-importing","title":"Metric exporting &amp; importing","text":"<p>When using the <code>local</code> indexer, it is possible to dump all of the collected metrics into a tarball, which you can import later. This is useful in disconnected environments, where kube-burner does not have direct access to an Elasticsearch instance. Metrics exporting can be configured by <code>createTarball</code> field of the indexer config as noted in the local indexer.</p> <p>The metric exporting feature is available through the <code>init</code> and <code>index</code> subcommands. Once you enabled it, a tarball (<code>kube-burner-metrics-&lt;timestamp&gt;.tgz</code>) containing all metrics is generated in the current working directory. This tarball can be imported and indexed by kube-burner with the <code>import</code> subcommand. For example:</p> <pre><code>$ kube-burner/bin/kube-burner import --config kubelet-config.yml --tarball kube-burner-metrics-1624441857.tgz\nINFO[2021-06-23 11:39:40] \ud83d\udcc1 Creating indexer: elastic\nINFO[2021-06-23 11:39:42] Importing tarball kube-burner-metrics-1624441857.tgz\nINFO[2021-06-23 11:39:42] Importing metrics from doc.json\nINFO[2021-06-23 11:39:43] Indexing [1] documents in kube-burner\nINFO[2021-06-23 11:39:43] Successfully indexed [1] documents in 208ms in kube-burner\n</code></pre>"},{"location":"observability/indexing/#scraping-from-multiple-endpoints","title":"Scraping from multiple endpoints","text":"<p>It is possible to scrape from multiple Prometheus endpoints and send the results to the target indexer with the <code>init</code> and <code>index</code> subcommands. This feature is configured by the flag <code>--metrics-endpoint</code>, which points to a YAML file with the required configuration.</p> <p>A valid file provided to the <code>--metrics-endpoint</code> looks like this:</p> <pre><code>- endpoint: http://localhost:9090 # This is one of the Prometheus endpoints\n  token: &lt;token&gt; # Authentication token\n  profile: metrics.yaml # Metrics profile to use in this target\n  alertProfile: alerts.yaml # Alert profile, optional\n- endpoint: http://remotehost:9090 # Another Prometheus endpoint\n  token: &lt;token&gt;\n  profile: metrics.yaml\n</code></pre> <p>Note</p> <p>The configuration provided by the <code>--metrics-endpoint</code> flag has precedence over the parameters specified in the config file. The <code>profile</code> and <code>alertProfile</code> parameters are optional. If not provided, they will be taken from the CLI flags.</p>"},{"location":"observability/metrics/","title":"Metric profile","text":"<p>The metric-collection feature is configured through a file pointed by the <code>metrics-profile</code> flag, which can point to a local path or URL of a YAML-formatted file containing a list of the Prometheus expressions. Kube-burner will perform those queries one by one, once all jobs are finished.</p> <p>In a single job benchmark, the queries are executed using the benchmark start and end time as time range. In multiple job benchmarks, these queries are executed in a per job basis, and they take the different start and end times from the executed jobs.</p> <p>The metrics profile file has the following structure:</p> <pre><code>- query: irate(process_cpu_seconds_total{job=~\".*(crio|etcd|controller-manager|apiserver|scheduler).*\"}[2m])\n  metricName: controlPlaneCPU\n\n- query: sum(irate(node_cpu_seconds_total[2m])) by (mode,instance)\n  metricName: nodeCPU\n</code></pre> <p>The <code>query</code> field holds the Prometheus expression to evaluate, and <code>metricName</code> controls the value that kube-burner will set on the <code>metricName</code> field of the generated documents. This is useful to identify metrics from a specific query. More information is available in the metric format section.</p>"},{"location":"observability/metrics/#instant-queries","title":"Instant queries","text":"<p>In addition to the default range queries, kube-burner has the ability execute instant queries against the provided Prometheus API. This can be enaled by enabling the field <code>instant</code> to the desired metric.</p> <pre><code>- query: kube_node_role\n  metricName: nodeRoles\n  instant: true\n</code></pre> <p>Info</p> <p>When using instant queries, at least two documents are generated, one resulting from scraping the last timestamp of the job, which would have the configued <code>metricName</code> field and an another one resulting from scraping the first timestamp of the job, the <code>metricName</code> of document is appended the <code>-start</code> suffix.</p>"},{"location":"observability/metrics/#metric-format","title":"Metric format","text":"<p>The collected metrics have the following shape:</p> <pre><code>[\n  {\n    \"timestamp\": \"2021-06-23T11:50:15+02:00\",\n    \"labels\": {\n      \"instance\": \"ip-10-0-219-170.eu-west-3.compute.internal\",\n      \"mode\": \"user\"\n    },\n    \"value\": 0.3300880234732172,\n    \"uuid\": \"&lt;UUID&gt;\",\n    \"query\": \"sum(irate(node_cpu_seconds_total[2m])) by (mode,instance) &gt; 0\",\n    \"metricName\": \"nodeCPU\",\n    \"jobConfig\": {\n      \"truncated_job_configuration\": \"foobar\"\n    }\n  },\n  {\n    \"timestamp\": \"2021-06-23T11:50:45+02:00\",\n    \"labels\": {\n      \"instance\": \"ip-10-0-219-170.eu-west-3.compute.internal\",\n      \"mode\": \"user\"\n    },\n    \"value\": 0.31978102677038506,\n    \"uuid\": \"&lt;UUID&gt;\",\n    \"query\": \"sum(irate(node_cpu_seconds_total[2m])) by (mode,instance) &gt; 0\",\n    \"metricName\": \"nodeCPU\",\n    \"jobConfig\": {\n      \"truncated_job_configuration\": \"foobar\"\n    }\n  }\n]\n</code></pre> <p>Notice that kube-burner enriches the query results by adding some extra fields like <code>uuid</code>, <code>query</code>, <code>metricName</code> and <code>jobName</code>.</p> <p>Info</p> <p>These extra fields are especially useful at the time of identifying and representing the collected metrics.</p>"},{"location":"observability/metrics/#using-the-elapsed-variable","title":"Using the elapsed variable","text":"<p>There is a special go-template variable that can be used within the Prometheus expressions of a metric profile; the variable <code>elapsed</code> is automatically populated with the job duration, in seconds. This variable is especially useful in PromQL expressions using aggregations over time functions.</p> <p>For example, the following expression gets the top 3 datapoints with the average CPU usage kubelets processes in the cluster.</p> <pre><code>- query: irate(process_cpu_seconds_total{service=\"kubelet\",job=\"kubelet\"}[2m]) * 100 and on (node) topk(3,avg_over_time(irate(process_cpu_seconds_total{service=\"kubelet\",job=\"kubelet\"}[2m])[{{ .elapsed }}:]))\n  metricName: top3KubeletCPU\n  instant: true\n</code></pre> <p>Info</p> <p>Note that in the [time-range:] notation, the colon specifies to get the values for the given duration.</p> <p>Examples of metrics profiles can be found in the examples directory. There are also Elasticsearch based Grafana dashboards available in the same examples directory.</p>"},{"location":"reference/configuration/","title":"Reference","text":"<p>All of the magic that <code>kube-burner</code> does is described in its configuration file. As previously mentioned, the location of this configuration file is provided by the flag <code>-c</code>. This flag points to a YAML-formatted file that consists of several sections.</p> <p>It is possible to use go-template semantics within this configuration file. It is also important to note that every environment variable is passed to this template, so we can reference them using the syntax <code>{{.MY_ENV_VAR}}</code>. For example, you could define the <code>indexerConfig</code> section of your own configuration file, such as:</p> <pre><code>type: elastic\nesServers: [{{ .ES_SERVER }}]\ndefaultIndex: elasticsearch-index\n</code></pre> <p>This feature can be very useful at the time of defining secrets, such as the user and password of our indexer, or a token to use in pprof collection.</p>"},{"location":"reference/configuration/#global","title":"Global","text":"<p>In this section is described global job configuration, it holds the following parameters:</p> Option Description Type Default <code>measurements</code> List of measurements. Detailed in the measurements section List [] <code>indexerConfig</code> Holds the indexer configuration. Detailed in the indexers section Object {} <code>requestTimeout</code> Client-go request timeout Duration 60s <code>gc</code> Garbage collect created namespaces Boolean false <code>gcMetrics</code> Flag to collect metrics during garbage collection Boolean false <code>gcTimeout</code> Garbage collection timeout Duration 1h <code>waitWhenFinished</code> Wait for all pods to be running when all jobs are completed Boolean false <p>Note</p> <p>The precedence order to wait on resources is Global.waitWhenFinished &gt; Job.waitWhenFinished &gt; Job.podWait</p> <p>kube-burner connects k8s clusters using the following methods in this order:</p> <ul> <li><code>KUBECONFIG</code> environment variable</li> <li><code>$HOME/.kube/config</code></li> <li>In-cluster config (Used when kube-burner runs inside a pod)</li> </ul>"},{"location":"reference/configuration/#jobs","title":"Jobs","text":"<p>This section contains the list of jobs <code>kube-burner</code> will execute. Each job can hold the following parameters.</p> Option Description Type Default <code>name</code> Job name String \"\" <code>jobType</code> Type of job to execute. More details at job types string create <code>jobIterations</code> How many times to execute the job Integer 0 <code>namespace</code> Namespace base name to use String \"\" <code>namespacedIterations</code> Whether to create a namespace per job iteration Boolean true <code>iterationsPerNamespace</code> The maximum number of <code>jobIterations</code> to create in a single namespace. Important for node-density workloads that create Services. Integer 1 <code>cleanup</code> Cleanup clean up old namespaces Boolean true <code>podWait</code> Wait for all pods to be running before moving forward to the next job iteration Boolean false <code>waitWhenFinished</code> Wait for all pods to be running when all iterations are completed Boolean true <code>maxWaitTimeout</code> Maximum wait timeout per namespace Duration 4h <code>jobIterationDelay</code> How long to wait between each job iteration. This is also the wait interval between each delete operation Duration 0s <code>jobPause</code> How long to pause after finishing the job Duration 0s <code>beforeCleanup</code> Allows to run a bash script before the workload is deleted String \"\" <code>qps</code> Limit object creation queries per second Integer 0 <code>burst</code> Maximum burst for throttle Integer 0 <code>objects</code> List of objects the job will create. Detailed on the objects section List [] <code>verifyObjects</code> Verify object count after running each job Boolean true <code>errorOnVerify</code> Set RC to 1 when objects verification fails Boolean true <code>skipIndexing</code> Skip metric indexing on this job Boolean false <code>preLoadImages</code> Kube-burner will create a DS before triggering the job to pull all the images of the job true <code>preLoadPeriod</code> How long to wait for the preload daemonset Duration 1m <code>preloadNodeLabels</code> Add node selector labels for the resources created in preload stage Object {} <code>namespaceLabels</code> Add custom labels to the namespaces created by kube-burner Object {} <code>churn</code> Churn the workload. Only supports namespace based workloads Boolean false <code>churnPercent</code> Percentage of the jobIterations to churn each period Integer 10 <code>churnDuration</code> Length of time that the job is churned for Duration 1h <code>churnDelay</code> Length of time to wait between each churn period Duration 5m <code>churnDeletionStrategy</code> Churn deletion strategy to apply, \"default\" or \"gvr\" (where <code>default</code> churns namespaces and <code>gvr</code> churns objects within namespaces) String default <p>Our configuration files strictly follow YAML syntax. To clarify on List and Object types usage, they are nothing but the <code>Lists and Dictionaries</code> in YAML syntax.</p> <p>Examples of valid configuration files can be found in the examples folder.</p>"},{"location":"reference/configuration/#objects","title":"Objects","text":"<p>The objects created by <code>kube-burner</code> are rendered using the default golang's template library. Each object element supports the following parameters:</p> Option Description Type Default <code>objectTemplate</code> Object template file path or URL String \"\" <code>replicas</code> How replicas of this object to create per job iteration Integer - <code>inputVars</code> Map of arbitrary input variables to inject to the object template Object - <code>wait</code> Wait for object to be ready Boolean true <code>waitOptions</code> Customize how to wait for object to be ready Object {} <code>runOnce</code> Create or delete this object only once during the entire job Boolean false <p>Warning</p> <p>Kube-burner is only able to wait for a subset of resources, unless <code>waitOptions</code> are specified.</p> <p>Info</p> <p>Find more info about the waiters implementation in the <code>pkg/burner/waiters.go</code> file</p>"},{"location":"reference/configuration/#wait-options","title":"Wait Options","text":"<p>If you want to override the default waiter behaviors, you can specify wait options for your objects.</p> Option Description Type Default <code>forCondition</code> Wait for the object condition with this name to be true String \"\" <p>For example, the snippet below can be used to make kube-burner wait for all containers from the pod defined at <code>pod.yml</code> to be ready.</p> <pre><code>objects:\n- objectTemplate: pod.yml\n  replicas: 3\n  waitOptions:\n    forCondition: Ready\n</code></pre>"},{"location":"reference/configuration/#default-labels","title":"Default labels","text":"<p>All objects created by kube-burner are labeled with <code>kube-burner-uuid=&lt;UUID&gt;,kube-burner-job=&lt;jobName&gt;,kube-burner-index=&lt;objectIndex&gt;</code>. They are used for internal purposes, but they can also be used by the users.</p>"},{"location":"reference/configuration/#job-types","title":"Job types","text":"<p>Configured by the parameter <code>jobType</code>, kube-burner supports three types of jobs with different parameters each.</p>"},{"location":"reference/configuration/#create","title":"Create","text":"<p>The default <code>jobType</code> is create. Creates objects listed in the <code>objects</code> list as described in the objects section. The amount of objects created is configured by <code>jobIterations</code>, <code>replicas</code>. If the object is namespaced and has an empty <code>.metadata.namespace</code> field, <code>kube-burner</code> creates a new namespace with the name <code>namespace-&lt;iteration&gt;</code>, and creates the defined amount of objects in it.</p>"},{"location":"reference/configuration/#delete","title":"Delete","text":"<p>This type of job deletes objects described in the objects list. Using delete as job type the objects list would have the following structure:</p> <pre><code>objects:\n- kind: Deployment\n  labelSelector: {kube-burner-job: cluster-density}\n  apiVersion: apps/v1\n\n- kind: Secret\n  labelSelector: {kube-burner-job: cluster-density}\n</code></pre> <p>Where:</p> <ul> <li><code>kind</code>: Object kind of the k8s object to delete.</li> <li><code>labelSelector</code>: Deletes the objects with the given labels.</li> <li><code>apiVersion</code>: API version from the k8s object.</li> </ul> <p>This type of job supports the following parameters. Some of them  are already described in the create job type section:</p> <ul> <li><code>waitForDeletion</code>: Wait for objects to be deleted before finishing the job. Defaults to <code>true</code>.</li> <li><code>name</code></li> <li><code>qps</code></li> <li><code>burst</code></li> <li><code>jobPause</code></li> <li><code>jobIterationDelay</code></li> </ul>"},{"location":"reference/configuration/#patch","title":"Patch","text":"<p>This type of job can be used to patch objects with the template described in the object list. This object list has the following structure:</p> <pre><code>objects:\n- kind: Deployment\n  labelSelector: {kube-burner-job: cluster-density}\n  objectTemplate: templates/deployment_patch_add_label.json\n  patchType: \"application/strategic-merge-patch+json\"\n  apiVersion: apps/v1\n</code></pre> <p>Where:</p> <ul> <li><code>kind</code>: Object kind of the k8s object to patch.</li> <li><code>labelSelector</code>: Map with the labelSelector.</li> <li><code>objectTemplate</code>: The YAML template or JSON file to patch.</li> <li><code>apiVersion</code>: API version from the k8s object.</li> <li><code>patchType</code>: The Kubernetes request patch type (see below).</li> </ul> <p>Valid patch types:</p> <ul> <li>application/json-patch+json</li> <li>application/merge-patch+json</li> <li>application/strategic-merge-patch+json</li> <li>application/apply-patch+yaml (requires YAML)</li> </ul> <p>As mentioned previously, all objects created by kube-burner are labeled with <code>kube-burner-uuid=&lt;UUID&gt;,kube-burner-job=&lt;jobName&gt;,kube-burner-index=&lt;objectIndex&gt;</code>. Therefore, you can design a workload with one job to create objects and another one to patch or remove the objects created by the previous.</p> <pre><code>jobs:\n- name: create-objects\n  namespace: job-namespace\n  jobIterations: 100\n  objects:\n  - objectTemplate: deployment.yml\n    replicas: 10\n\n  - objectTemplate: service.yml\n    replicas: 10\n\n- name: remove-objects\n  jobType: delete\n  objects:\n  - kind: Deployment\n    labelSelector: {kube-burner-job: create-objects}\n    apiVersion: apps/v1\n\n  - kind: Secret\n    labelSelector: {kube-burner-job: create-objects}\n</code></pre>"},{"location":"reference/configuration/#churning-jobs","title":"Churning Jobs","text":"<p>Churn is the deletion and re-creation of objects, and is supported for namespace-based jobs only. This occurs after the job has completed but prior to uploading metrics, if applicable. It deletes a percentage of contiguous namespaces randomly chosen and re-creates them with all of the appropriate objects. It will then wait for a specified delay (or none if set to <code>0</code>) before deleting and recreating the next randomly chosen set. This cycle continues until the churn duration has passed.</p> <p>An example implementation that would churn 20% of the 100 job iterations for 2 hours with no delay between sets:</p> <pre><code>jobs:\n- name: cluster-density\n  jobIterations: 100\n  namespacedIterations: true\n  namespace: churning\n  churn: true\n  churnPercent: 20\n  churnDuration: 2h\n  churnDelay: 0s\n  objects:\n  - objectTemplate: deployment.yml\n    replicas: 10\n\n  - objectTemplate: service.yml\n    replicas: 10\n</code></pre>"},{"location":"reference/configuration/#injected-variables","title":"Injected variables","text":"<p>All object templates are injected with the variables below by default:</p> <ul> <li><code>Iteration</code>: Job iteration number.</li> <li><code>Replica</code>: Object replica number. Keep in mind that this number is reset to 1 with each job iteration.</li> <li><code>JobName</code>: Job name.</li> <li><code>UUID</code>: Benchmark UUID.</li> </ul> <p>In addition, you can also inject arbitrary variables with the option <code>inputVars</code> of the object:</p> <pre><code>- objectTemplate: service.yml\n  replicas: 2\n  inputVars:\n    port: 80\n    targetPort: 8080\n</code></pre> <p>The following code snippet shows an example of a k8s service using these variables:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: sleep-app-{{.Iteration}}-{{.Replica}}\n  labels:\n    name: my-app-{{.Iteration}}-{{.Replica}}\nspec:\n  selector:\n    app: sleep-app-{{.Iteration}}-{{.Replica}}\n  ports:\n  - name: serviceport\n    protocol: TCP\n    port: \"{{.port}}\"\n    targetPort: \"{{.targetPort}}\"\n  type: ClusterIP\n</code></pre> <p>You can also use golang template semantics in your <code>objectTemplate</code> definitions</p> <pre><code>kind: ImageStream\napiVersion: image.openshift.io/v1\nmetadata:\n  name: {{.prefix}}-{{.Replica}}\nspec:\n{{ if .image }}\n  dockerImageRepository: {{.image}}\n{{ end }}\n</code></pre>"},{"location":"reference/configuration/#template-functions","title":"Template functions","text":"<p>In addition to the default golang template semantics, kube-burner is compiled with the sprig library, which adds over 70 template functions for Go\u2019s template language.</p>"},{"location":"reference/configuration/#runonce","title":"RunOnce","text":"<p>All objects within the job will iteratively run based on the JobIteration number,  but there may be a situation if an object need to be created only once (ex. clusterrole), in such cases we can add an optional field as <code>runOnce</code> for that particular object to execute only once in the entire job.</p> <p>An example scenario as below template, a job iteration of 100 but create the clusterrole only once.</p> <pre><code>jobs:\n- name: cluster-density\n  jobIterations: 100\n  namespacedIterations: true\n  namespace: cluster-density\n  objects:\n  - objectTemplate: clusterrole.yml\n    replicas: 1\n    runOnce: true\n\n  - objectTemplate: clusterrolebinding.yml\n    replicas: 1\n    runOnce: true\n\n  - objectTemplate: deployment.yml\n    replicas: 10\n</code></pre>"}]}